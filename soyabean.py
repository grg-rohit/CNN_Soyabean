# -*- coding: utf-8 -*-
"""Soyabean.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12SEs6t9PKLFcM4vVop5h70SIwjASdT3v

#Import
"""

!pip install ucimlrepo

from ucimlrepo import fetch_ucirepo, list_available_datasets

list_available_datasets()

soyabean = fetch_ucirepo(id=90)

X = soyabean.data.features
y = soyabean.data.targets

print(soyabean.metadata)

print(soyabean.variables)

X.describe()

X.head()

y.describe()

y.value_counts()

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()
y = le.fit_transform(y)

y

import pandas as pd
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

"""#Model

"""

from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

#Data Preprocessing
imputer_X = SimpleImputer(strategy='mean')
X_imputed = imputer_X.fit_transform(X)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# plitting Data into Train and Test Sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.4, random_state=42)


class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        # Define the hidden layers
        self.input_to_hidden_layer = nn.Linear(35, 100)
        self.hidden_layer_activation = nn.ReLU()
        self.hidden1_to_hidden2 = nn.Linear(100, 100)
        self.hidden2_activation = nn.ReLU()
        self.hidden2_to_hidden3 = nn.Linear(100, 150)
        self.hidden3_activation = nn.ReLU()
        self.hidden3_to_hidden4 = nn.Linear(150, 150)
        self.hidden4_activation = nn.ReLU()
        self.hidden4_to_hidden5 = nn.Linear(150, 200)
        self.hidden5_activation = nn.ReLU()
        self.hidden5_to_hidden6 = nn.Linear(200, 200)
        self.hidden6_activation = nn.ReLU()
        self.hidden6_to_hidden7 = nn.Linear(200, 200)
        self.hidden7_activation = nn.ReLU()
        # Define output layer
        self.hidden_to_output = nn.Linear(200, len(np.unique(y)))
        self.output_layer_activation = nn.Softmax(dim=1)  # Softmax for multi-class classification

    def forward(self, x):
        x = self.input_to_hidden_layer(x)
        x = self.hidden_layer_activation(x)
        x = self.hidden1_to_hidden2(x)
        x = self.hidden2_activation(x)
        x = self.hidden2_to_hidden3(x)
        x = self.hidden3_activation(x)
        x = self.hidden3_to_hidden4(x)
        x = self.hidden4_activation(x)
        x = self.hidden4_to_hidden5(x)
        x = self.hidden5_activation(x)
        x = self.hidden5_to_hidden6(x)
        x = self.hidden6_activation(x)
        x = self.hidden6_to_hidden7(x)
        x = self.hidden7_activation(x)
        x = self.hidden_to_output(x)
        x = self.output_layer_activation(x)
        return x

#Model
model = NeuralNetwork()
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=0.0003)  # Example learning rate

#Training
num_epochs = 2000
for epoch in range(num_epochs):
    # Forward pass
    outputs = model(torch.tensor(X_train, dtype=torch.float32))
    loss = criterion(outputs, torch.tensor(y_train, dtype=torch.long))

    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Evaluate model on validation set
    with torch.no_grad():
        model.eval()
        val_outputs = model(torch.tensor(X_test, dtype=torch.float32))
        _, predicted_classes = torch.max(val_outputs, 1)
        val_accuracy = accuracy_score(y_test, predicted_classes.numpy())
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy:.4f}")
        model.train()

#Evaluate final model on test set
with torch.no_grad():
    model.eval()
    test_outputs = model(torch.tensor(X_test, dtype=torch.float32))
    _, predicted_classes = torch.max(test_outputs, 1)
    test_accuracy = accuracy_score(y_test, predicted_classes.numpy())
    print(f"Final Test Accuracy: {test_accuracy:.4f}")

"""#Plots

"""

import matplotlib
import matplotlib.pyplot as plt

plt.plot(epoch_loss)
plt.title("Loss over epoch")
plt.xlabel("Epochs")
plt.ylabel("Epochs")

plt.plot(epoch_accuracy)
plt.title("Accuracy over epoch")
plt.xlabel("Epochs")
plt.ylabel("Epochs")

"""#Evaluation on Test Data"""

X_test_tensor = torch.tensor(X_test, dtype = torch.float32)
y_test_tensor = torch.tensor(y_test, dtype = torch.long)

with torch.no_grad():
  model.eval()
  test_outputs = model(X_test_tensor)
  _,predicted_test_classes = torch.max(test_outputs,1)

from sklearn.metrics import accuracy_score, confusion_matrix

test_accuracy = accuracy_score(y_test_tensor, predicted_test_classes)
print(f"Test Accuracy: {test_accuracy}")

import seaborn as sns

conf_matrix = confusion_matrix(y_test, predicted_test_classes)

conf_matrix